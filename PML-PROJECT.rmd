---
title: "Practical Machine Learning - Course Project"
author: "LF GOFFIN"
date: "21 octobre 2015"
output: html_document
---

#Introduction
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify *how well they do it*. In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of our project is to predict the manner in which they did the exercise, the "classe" variable in the training set. 
More information is available from the website here: (http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)=.
The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

#Data Pre-Processing
##Load required libraries
Let's first load the needed Libraries :
```{r, message=FALSE, warning=FALSE, comment=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(utils)
```

##Download the Data
Then let's download the training and test datasets :
```{r Download_data, cache=FALSE}
pathIn <- file.path("./PMLdata")
trainurl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainfile <- paste(pathIn,"pml-training.csv",sep="/")
testfile  <- paste(pathIn,"pml-testing.csv",sep="/")
if (!file.exists("PML/PMLdata")) {
  dir.create("PML/PMLdata")
}
if (!file.exists(trainfile)) {
  download.file(trainurl, destfile=trainfile)
}
if (!file.exists(testfile)) {
  download.file(testurl, destfile=testfile)
}
```

##Read the Data
Let's put the data in two dataframes, and evaluate the respective number of entries and variables :
```{r Read_data, message=FALSE, warning=FALSE}
traindata <- read.csv(trainfile)
testdata <- read.csv(testfile)
```
The training data set contains at this stage `r dim(traindata)[1]` observations and `r dim(traindata)[2]` variables; the testing data set contains `r dim(testdata)[1]` observations and `r dim(testdata)[2]` variables. The "classe" variable is the outcome we want to predict.

##Clean the Data
Let's clean the datasets. There are missing and empty values we want to remove from the datasets :
```{r Clean_data, message=FALSE, warning=FALSE}
traindata[traindata==""] <- NA
testdata[testdata==""] <- NA
traindata <- traindata[, colSums(is.na(traindata)) == 0] 
testdata <- testdata[, colSums(is.na(testdata)) == 0] 

trainRemove <- grepl("^X|timestamp|window", names(traindata))
traindata <- traindata[, !trainRemove]
testRemove <- grepl("^X|timestamp|window", names(testdata))
testdata <- testdata[, !testRemove]
```
This operation drastically reduces the number of variables down to `r dim(traindata)[2]`. This will significantly improve the speed performances of models performed in the next steps of the analysis

##Split the Data
Let's create a **70/30 partition** from the training data for cross-validation purpose : 
```{r Split_data, message=FALSE, warning=FALSE}
set.seed(12345) # For reproducibility purpose
inTrain <- createDataPartition(traindata$classe, p=0.70, list=F)
traindata <- traindata[inTrain, ]
probedata <- traindata[-inTrain, ]
```


#Data Modeling
To improve the reliability of our study, we will here compare the results from a *Tree Model* and from a *Random Forest Model*, and then choose the best one.
##Tree Model
```{r Tree_Model, message=FALSE, warning=FALSE}
treeModel <- train(classe ~ ., data=traindata, method="rpart")
treeModel
```
The respective Tree Plot is provided in the **Appendix** section.

##Random Forest Model
We decide to use a **5-fold cross** validation when applying the algorithm :
```{r RF_Model, message=FALSE, warning=FALSE}
contRf <- trainControl(method="cv", 5)
RFModel <- train(classe ~ ., data=traindata, method="rf", trControl=contRf, ntree=250)
RFModel
```

#Cross-Validation
Let's apply our respective models on the validation datasets :
##Tree Model Validation
```{r Tree_Val, message=FALSE, warning=FALSE}
treePredict <- predict(treeModel, probedata)
confusionMatrix(probedata$classe, treePredict)
confusionMatrix(probedata$classe, treePredict)$overall[1]
```

##Random Forest Model Validation
```{r RF_Val, message=FALSE, warning=FALSE}
RFPredict <- predict(RFModel, probedata)
confusionMatrix(probedata$classe, RFPredict)
confusionMatrix(probedata$classe, RFPredict)$overall[1]
```

> Comparing the accuracy results above, we can see that **Random Forest Model** is much more reliable than the Tree Model. We decide to keep this one to predict the "class" variable" from the *Test* dataset.

```{r RF_eoos, message=FALSE, warning=FALSE}
1-as.numeric(confusionMatrix(probedata$classe, RFPredict)$overall[1])
```
Base upon the accuracy calculated above the the estimated out-of-sample error is `r 1-as.numeric(confusionMatrix(probedata$classe, RFPredict)$overall[1])`.

For information, a plot of the *Top 25* variables is provided in the **Appendix** section.  

#Prediction for Test dataset
```{r Prediction, message=FALSE, warning=FALSE}
predict(RFModel, testdata)
```

#Appendix

##Tree Model Plot
```{r Appendix_A, message=FALSE, warning=FALSE}
prp(rpart(classe ~ ., data=traindata, method="class"))
```

##Top 25 Most Important Variables
```{r Appendix_B, message=FALSE, warning=FALSE}
plot(varImp(RFModel), main = "Variable Importance : Top 25", top = 25)
```